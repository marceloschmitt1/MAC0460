{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão linear 2:  gradiente descendente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendizado e otimização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada uma função *target* desconhecida $f:\\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ e uma hipótese $h_{\\mathbf{w}}:\\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ o erro de $h_{\\mathbf{w}}$ é definido por:\n",
    "\n",
    "\\begin{equation}\n",
    "E_{out}(h_{\\mathbf{w}}) = \\mathbb{E}_{\\mathbf{x}\\sim p_{data}}L(h(\\mathbf{x}; \\mathbf{w}), \\; f(\\mathbf{x}))\n",
    "\\end{equation}\n",
    "\n",
    "em que $p_{data}$ é a distribuição geradora dos dados, $L$  é alguma função de perda (por exemplo, o quadrado da diferença) e $h(\\mathbf{x}; \\mathbf{w}) = h_{\\mathbf{w}}(\\mathbf{x})$. Se tivéssemos acesso a $p_{data}$ poderíamos calcular a função acima para qualquer $h_{\\mathbf{w}}$ e escolher aquele com erro mínimo.\n",
    "\n",
    "Como não temos acesso a $p_{data}$, define-se\n",
    "\n",
    "\\begin{equation}\n",
    "E_{in}(h_{\\mathbf{w}}) = J(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^{N} L(h(\\mathbf{x}_{i}; \\mathbf{w}), \\; y_{i})\n",
    "\\end{equation}\n",
    "\n",
    "em que $N$ é o tamanho do dataset de treinamento e $y_{i} = f(\\mathbf{x}_{i})$.\n",
    "\n",
    "Como visto em aula, uma relação entre $E_{in}$ e $E_{out}$ pode ser estabelecida pela **Inequação de Hoeffding**.\n",
    "\n",
    "Aqui estamos interessados em encontrar $h_{\\mathbf{w}}$ com erro $J(\\mathbf{w})$ mínimo. Isto corresponde a determinar o ponto mínimo da função $J(\\mathbf{w})$. Por isso, vamos nos concentrar apenas em uma técnica de otimização para minimizar $E_{in}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente ascendente e gradiente descendente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dados $\\mathbf{w}, \\mathbf{u} \\in \\mathbb{R}^{d}$ e $J:\\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ tal que $||\\mathbf{u}||_{2} = 1$ a taxa de variação de $J$ no ponto $\\mathbf{w}$ em direção a $\\mathbf{u}$ é chamada de **derivada direcional**, $D_{\\mathbf{u}}J(\\mathbf{w})$. Definindo $g(h) = J(\\mathbf{w} + h\\mathbf{u})$, podemos usar a regra da cadeia e a definição de produto escalar para mostrar que  \n",
    "\n",
    "\\begin{equation}\n",
    "D_{\\mathbf{u}}J(\\mathbf{w}) \\;\\;=\\;\\; \\left. \\frac{dg}{dh} \\right|_{h=0} \\;\\;=\\;\\; \\mathbf{u}^{T}\\nabla_{\\mathbf{w}}J(\\mathbf{w})\n",
    "    \\;\\;=\\;\\; ||\\mathbf{u}||_{2}||\\nabla_{\\mathbf{w}}J(\\mathbf{w})||_{2}cos\\theta \\;\\;=\\;\\; ||\\nabla_{\\mathbf{w}}J(\\mathbf{w})||_{2}cos\\theta\n",
    "\\end{equation}\n",
    "\n",
    "em que $\\theta$ é o ângulo entre $\\mathbf{u}$ e $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$. Assim, pensando em $D_{\\mathbf{u}}J(\\mathbf{w})$ em termos do vetor $\\mathbf{u}$ temos que: $D_{\\mathbf{u}}J(\\mathbf{w})$ tem o valor máximo quando $\\mathbf{u}$ tem a mesma direção que $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ ($\\theta=0$); similarmente $D_{\\mathbf{u}}J(\\mathbf{w})$ tem o valor mínimo quando $\\mathbf{u}$ tem a direção oposta a $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ ($\\theta=\\pi$).\n",
    "\n",
    "Desse modo podemos maximizar $J$ alterando $\\mathbf{w}$ na direção de $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ e podemos minimizar $J$ alterando $\\mathbf{w}$ na direção de $-\\nabla_{\\mathbf{w}}J(\\mathbf{w})$. Isso permite dois métodos bem simples de otimização:\n",
    "\n",
    "**Gradiente Ascendente**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * $\\mathbf{w}(t+1) = \\mathbf{w}(t) + \\eta \\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$ \n",
    "\n",
    "    \n",
    "\n",
    "**Gradiente Descendente**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * $\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$ \n",
    "\n",
    "Em ambos os casos o parâmetro $\\eta \\in \\mathbb{R}_{\\geq}$ é chamado de **taxa de aprendizado** (*learning rate*); ele pondera o tamanho de cada atualização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   A seguir são apresentados uma sequência de exercícios que ilustram diferentes aspectos práticos na implementação do algoritmo *gradient descent*  \n",
    "Iremos considerar o problema de regressão linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all imports\n",
    "import numpy as np\n",
    "import time\n",
    "from util import r_squared, randomize_in_place, get_housing_prices_data\n",
    "from plots import simple_step_plot, plot_points_regression, plot_cost_function_curve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Vamos usar o mesmo dataset de antes, mas agora vamos dividir os dados em treinamento, validação e teste.\n",
    "Essa divisão é comumente realizada na prática: os dados de treinamento são usados na otimização da função custo $J$, os dados de validação são usados para aferir a qualidade da otimização, e os dados de teste são usados para aferir a qualidade da predição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = get_housing_prices_data(N=350, verbose=False)\n",
    "randomize_in_place(X, y)\n",
    "\n",
    "train_X = X[0:250]\n",
    "train_y = y[0:250]\n",
    "valid_X = X[250:300]\n",
    "valid_y = y[250:300]\n",
    "test_X = X[300:]\n",
    "test_y = y[300:]\n",
    "\n",
    "print(\"\\nDividindo os dados em treinamento, validação e teste\")\n",
    "print(\"\\ntrain_X shape = {}\".format(train_X.shape))\n",
    "print(\"\\ntrain_y shape = {}\".format(train_y.shape))\n",
    "print(\"\\nvalid_X shape = {}\".format(valid_X.shape))\n",
    "print(\"\\nvalid_y shape = {}\".format(valid_y.shape))\n",
    "print(\"\\ntest_X shape = {}\".format(test_X.shape))\n",
    "print(\"\\ntest_y shape = {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_points_regression(train_X,\n",
    "                       train_y,\n",
    "                       title='Training data',\n",
    "                       xlabel=\"m\\u00b2\",\n",
    "                       ylabel='$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para minimizar a função de custo vamos ter que colocar os dados em uma outra escala\n",
    "\n",
    "Um modo de fazer isso é o chamado [standard/z score](https://en.wikipedia.org/wiki/Standard_score).\n",
    "Aplicamos a seguinte transformação: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X}^{\\top}_{i} \\leftarrow \\frac{\\mathbf{X}^{\\top}_{i} - \\mu_{i}}{\\sigma_{i}}\n",
    "\\end{equation}\n",
    "\n",
    "em que $\\mathbf{X}^{T}_{i} \\in \\mathbb{R}^{N}$ ($i = 1, \\dots, d$) é um vetor de features da design matrix $\\mathbf{X}$, $\\mu_{i}$ é a média de tal vetor, e $\\sigma_{i}$ seu desvio padrão.\n",
    "\n",
    "A importância de se fazer essa transformação é discutida mais ao final deste notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 1)** \n",
    "Use a biblioteca numpy para implementar a função que altera os dados conforme a equação acima (essa função deve funcionar para uma design matrix $\\mathbf{X}$ com um número arbitrário de features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    \"\"\"\n",
    "    Returns standardized version of the ndarray 'X'.\n",
    "\n",
    "    :param X: input array\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :return: standardized array\n",
    "    :rtype: np.ndarray(shape=(N, d))\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError(\"falta completar a função standardize\")\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return X_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toy_X = np.array([[1100.3, 2.4, 34.34],\n",
    "                  [2300.3, 1.4, 442.23]])\n",
    "toy_y = np.array([[1000.2], [2000.5]])\n",
    "toy_X_norm = standardize(toy_X)\n",
    "toy_y_norm = standardize(toy_y)\n",
    "xmean, xstd = np.mean(toy_X_norm), np.std(toy_X_norm)\n",
    "ymean, ystd = np.mean(toy_y_norm), np.std(toy_y_norm)\n",
    "assert -1 <= xmean < 0\n",
    "assert 0 <= ymean < 1\n",
    "assert 0.9 <= xstd <= 1\n",
    "assert 0.9 <= ystd <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X_norm = standardize(train_X)\n",
    "train_y_norm = standardize(train_y)\n",
    "\n",
    "\n",
    "xmean, xstd = np.mean(train_X), np.std(train_X)\n",
    "xmax, xmin = np.max(train_X), np.min(train_X)\n",
    "ymean, ystd = np.mean(train_y), np.std(train_y)\n",
    "ymax, ymin = np.max(train_y), np.min(train_y)\n",
    "\n",
    "print(\"Dados originais\\n\")\n",
    "print(\"X:\\nmean {}, std {:.2f}, max {}, min {}\".format(xmean,\n",
    "                                                       xstd,\n",
    "                                                       xmax,\n",
    "                                                       xmin))\n",
    "print(\"\\ny:\\nmean {}, std {:.2f}, max {}, min {}\\n\".format(ymean,\n",
    "                                                         ystd,\n",
    "                                                         ymax,\n",
    "                                                         ymin))\n",
    "\n",
    "\n",
    "xmean, xstd = np.mean(train_X_norm), np.std(train_X_norm)\n",
    "xmax, xmin = np.max(train_X_norm), np.min(train_X_norm)\n",
    "ymean, ystd = np.mean(train_y_norm), np.std(train_y_norm)\n",
    "ymax, ymin = np.max(train_y_norm), np.min(train_y_norm)\n",
    "\n",
    "print(\"Dados normalizados\\n\")\n",
    "print(\"X:\\nmean {}, std {:.2f}, max {}, min {}\".format(xmean,\n",
    "                                                       xstd,\n",
    "                                                       xmax,\n",
    "                                                       xmin))\n",
    "print(\"\\ny:\\nmean {}, std {:.2f}, max {}, min {}\\n\".format(ymean,\n",
    "                                                         ystd,\n",
    "                                                         ymax,\n",
    "                                                         ymin))\n",
    "plot_points_regression(train_X_norm,\n",
    "                       train_y_norm,\n",
    "                       title='Training data (normalized)',\n",
    "                       xlabel=\"m\\u00b2\",\n",
    "                       ylabel='$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Adicionando uma componente com apenas 1s como uma nova feature\n",
    "Conforme já vimos, adicionar uma componente (coordenada artificial) constante 1 é conveniente. Isto é, em vez de $\\mathbf{x} \\in \\mathbb{R}^d$ é conveniente considerarmos $(1,\\mathbf{x}) \\in \\mathbb{R}^{d+1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_feature_ones(X):\n",
    "    \"\"\"\n",
    "    Returns the ndarray 'X' with the extra\n",
    "    feature column containing only 1s.\n",
    "\n",
    "    :param X: input array\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :return: output array\n",
    "    :rtype: np.ndarray(shape=(N, d+1))\n",
    "    \"\"\"\n",
    "    return np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "train_X_1 = add_feature_ones(train_X_norm)\n",
    "print(\"\\ntrain_X shape = {}\".format(train_X_1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Criando a predição da regressão linear e plotando uma predição arbitrária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_regression_prediction(X, w):\n",
    "    \"\"\"\n",
    "    Calculates the linear regression prediction.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :return: prediction\n",
    "    :rtype: np.array(shape=(N, 1))\n",
    "    \"\"\"\n",
    "\n",
    "    return X.dot(w)\n",
    "\n",
    "w = np.array([[1.2], [2.3]])\n",
    "prediction = linear_regression_prediction(train_X_1, w)\n",
    "\n",
    "plot_points_regression(train_X_norm,\n",
    "                       train_y_norm,\n",
    "                       title='Training data (normalized)',\n",
    "                       xlabel=\"m\\u00b2\",\n",
    "                       ylabel='$',\n",
    "                       prediction=prediction,\n",
    "                       legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computando a função de custo\n",
    "\n",
    "Usando o erro quadrárico médio, a função de custo $J(\\mathbf{w})$ para a tarefa de regressão linear pode ser escrita de dois modos. A forma iterativa:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^{N}(\\hat{y}_{i} - y_{i})^{2}\n",
    "\\end{equation}\n",
    "\n",
    "e a forma vetorial:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{w}) = \\frac{1}{N}(\\mathbf{X}\\mathbf{w} - \\mathbf{y})^{T}(\\mathbf{X}\\mathbf{w} - \\mathbf{y})\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 2)**  \n",
    "Use a biblioteca numpy para implementar a função de custo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w):\n",
    "    \"\"\"\n",
    "    Calculates  mean square error cost.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :return: cost\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError(\"falta completar a função compute_cost\")\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toy_w = np.array([[1], [1], [2]])\n",
    "toy_X = np.array([[2, 3, 1],\n",
    "                  [5, 1, 2]])\n",
    "toy_y = np.array([[1], [1]])\n",
    "assert compute_cost(toy_X, toy_y, toy_w) == 58.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos olhar a superficie de custo e ver onde se situa um valor $J(\\mathbf{w})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_w = np.array([[15], [-35.3]])\n",
    "initial_J = compute_cost(train_X_1, train_y_norm, initial_w)\n",
    "\n",
    "plot_cost_function_curve(train_X_1,\n",
    "                         train_y_norm,\n",
    "                         compute_cost,\n",
    "                         title=\"Optimization landscape\",\n",
    "                         weights_list=[initial_w.flatten()],\n",
    "                         cost_list=[initial_J])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando os gradientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É fácil calcular a derivada parcial de $J(\\mathbf{w})$ com relação a cada entrada $j$ de $\\mathbf{w}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}_{j}} = \\frac{2}{N}\\sum_{i=1}^{N} (\\hat{y}_i - y_i) \\mathbf{x}_{ij}\n",
    "\\end{equation}\n",
    "\n",
    "Lembre que o gradiente de $J(\\mathbf{w})$ com relação a $\\mathbf{w}$ é:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\mathbf{w}}J(\\mathbf{w}) = \\begin{bmatrix}\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}_{1}} \\dots \\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}_{m}} \\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 3)**  \n",
    "Use a biblioteca numpy para calcular $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_wgrad(X, y, w):\n",
    "    \"\"\"\n",
    "    Calculates gradient of J(w) with respect to w.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :return: gradient\n",
    "    :rtype: np.array(shape=(d, 1))\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError(\"falta completar a função compute_wgrad\")\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad_check(X, y, w, h=1e-4):\n",
    "    \"\"\"\n",
    "    Check gradients for linear regression.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :param h: small variation\n",
    "    :type h: float\n",
    "    :return: gradient test\n",
    "    :rtype: boolean\n",
    "    \"\"\"\n",
    "    Jw = compute_cost(X, y, w)\n",
    "    grad = compute_wgrad(X, y, w)\n",
    "    passing = True\n",
    "    d = w.shape[0]\n",
    "    for i in range(d):\n",
    "        w_plus_h = np.array(w, copy=True)\n",
    "        w_plus_h[i] = w_plus_h[i] + h\n",
    "        Jw_plus_h = compute_cost(X, y, w_plus_h)\n",
    "        w_minus_h = np.array(w, copy=True)\n",
    "        w_minus_h[i] = w_minus_h[i] - h\n",
    "        Jw_minus_h = compute_cost(X, y, w_minus_h)\n",
    "        numgrad_i = (Jw_plus_h - Jw_minus_h) / (2 * h)\n",
    "        reldiff = abs(numgrad_i - grad[i]) / max(1, abs(numgrad_i), abs(grad[i]))\n",
    "        if reldiff > 1e-5:\n",
    "            passing = False\n",
    "            msg = \"\"\"\n",
    "            Seu gradiente = {0}\n",
    "            Gradiente numérico = {1}\"\"\".format(grad[i], numgrad_i)\n",
    "            print(\"            \" + str(i) + \": \" + msg)\n",
    "            print(\"            Jw = {}\".format(Jw))\n",
    "            print(\"            Jw_plus_h = {}\".format(Jw_plus_h))\n",
    "            print(\"            Jw_minus_h = {}\\n\".format(Jw_minus_h))\n",
    "\n",
    "    if passing:\n",
    "        print(\"Gradiente passando!\")\n",
    "    \n",
    "    return passing \n",
    "\n",
    "toy_w1 = np.array([[1.], [2.], [1.], [2.]])\n",
    "toy_X1 = np.array([[2., 3., 1., 2.],\n",
    "                  [5., 1., 1., 2.]])\n",
    "toy_y1 = np.array([[1.], [-1.]])\n",
    "toy_w2 = np.array([[-100.22], [20002.1], [102.5]])\n",
    "toy_X2 = np.array([[2111.3, -2223., 404.0],\n",
    "                  [5222., -22221., 3.3]])\n",
    "toy_y2 = np.array([[122.], [221.]])\n",
    "toy_w3 = np.array([[-10.22], [-3.1]])\n",
    "toy_X3 = np.array([[1.3, -1.2],\n",
    "                  [2.2, -2.1],\n",
    "                  [-2.3, -5.5],\n",
    "                  [3.2, 8.1],\n",
    "                  [3.3, -1.1],\n",
    "                  [-3.4, -2.22],\n",
    "                  [2.23, -4.4],\n",
    "                  [5.2, -2.3]])\n",
    "toy_y3 = np.array([[10.3],\n",
    "                   [23.3],\n",
    "                   [10.1],\n",
    "                   [-20.2],\n",
    "                   [-10.2],\n",
    "                   [20.2],\n",
    "                   [-14.4],\n",
    "                   [-30.3]])\n",
    "\n",
    "assert grad_check(toy_X1, toy_y1, toy_w1)\n",
    "assert grad_check(toy_X2, toy_y2, toy_w2)\n",
    "assert grad_check(toy_X3, toy_y3, toy_w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch gradient descent\n",
    "\n",
    "A versão mais simples do algoritmo *gradient descent* faz uso de todas as observações do dataset de treinamento (esse algoritmo também é conhecido como *batch gradient descent* ou *vanilla gradient descent*).\n",
    "\n",
    "**Batch gradient descent**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * Compute the gradient $\\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$ \n",
    "    * Apply update : $\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 4)** \n",
    "Implemente o algoritmo batch gradient descent com a taxa de apreendizado fixa para a regressão linear. A função abaixo deve retornar três coisas: o vetor de pesos $\\mathbf{w}$, uma lista com cada peso obtido ao longo do treinamento, e uma lista com o custo de cada peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X, y, w, learning_rate, num_iters):\n",
    "    \"\"\"\n",
    "     Performs batch gradient descent optimization.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :param learning_rate: learning rate\n",
    "    :type learning_rate: float\n",
    "    :param num_iters: number of iterations\n",
    "    :type num_iters: int\n",
    "    :return: weights, weights history, cost history\n",
    "    :rtype: np.array(shape=(d, 1)), list, list\n",
    "    \"\"\"\n",
    "    \n",
    "    weights_history = [w.flatten()]\n",
    "    cost_history = [compute_cost(X, y, w)]\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError(\"falta completar a função batch_gradient_descent\")\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return w, weights_history, cost_history "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 4)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.8\n",
    "iterations = 20000\n",
    "init = time.time()\n",
    "w, weights_history, cost_history = batch_gradient_descent(train_X_1,\n",
    "                                                          train_y_norm,\n",
    "                                                          initial_w,\n",
    "                                                          learning_rate,\n",
    "                                                          iterations)\n",
    "assert cost_history[-1] < cost_history[0]\n",
    "assert type(w) == np.ndarray\n",
    "assert len(weights_history) == len(cost_history)\n",
    "init = time.time() - init\n",
    "print(\"Tempo de treinamento = {:.8f}(s)\".format(init))\n",
    "print(\"Tem que ser em menos de 1 segundo \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Agora podemos treinar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.03\n",
    "iterations = 400\n",
    "w, weights_history, cost_history = batch_gradient_descent(train_X_1,\n",
    "                                                          train_y_norm,\n",
    "                                                          initial_w,\n",
    "                                                          learning_rate,\n",
    "                                                          iterations)\n",
    "title = \"Optimization landscape\\nlearning rate = {} | iterations = {}\".format(learning_rate,\n",
    "                                                                              iterations)\n",
    "plot_cost_function_curve(train_X_1,\n",
    "                         train_y_norm,\n",
    "                         compute_cost,\n",
    "                         title=title,\n",
    "                         weights_list=weights_history,\n",
    "                         cost_list=cost_history)\n",
    "simple_step_plot([cost_history],\n",
    "             \"loss\",\n",
    "             'Training loss\\nlearning rate = {} | iterations = {}'.format(learning_rate,\n",
    "                                                                          iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiper parâmetros (*hyperparameters*)\n",
    "\n",
    "\n",
    "Hiper parâmetros são parâmetros que controlam o comportamento do algoritmo. Eles não são modificados pelo algoritmo de aprendizado. Escolhemos os hiper parâmetros de acordo com a performance deles no dataset de treinamento. Para evitar que o modelo decore o dataset de treinamento, pegamos uma parte desse dataset só para achar os melhores hiper parâmetros. Essa parte é chamada de **dataset de validação** (*validation set*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "valid_X_norm = standardize(valid_X)\n",
    "valid_y_norm = standardize(valid_y)\n",
    "valid_X_1 = add_feature_ones(valid_X_norm)\n",
    "\n",
    "hyper_params = [(0.001, 200),\n",
    "                (0.1, 10),\n",
    "                (0.9, 8),\n",
    "                (0.02, 600)]\n",
    "\n",
    "all_costs = []\n",
    "all_w = []\n",
    "\n",
    "for param in hyper_params:\n",
    "    learning_rate = param[0]\n",
    "    iterations = param[1]\n",
    "    w, weights_history, cost_history = batch_gradient_descent(train_X_1,\n",
    "                                                              train_y_norm,\n",
    "                                                              initial_w,\n",
    "                                                              learning_rate,\n",
    "                                                              iterations)\n",
    "    all_costs.append(compute_cost(valid_X_1, valid_y_norm, w))\n",
    "    all_w.append(w)\n",
    "    title = \"Optimization landscape\\n\"\n",
    "    title += \"learning rate = {} | iterations = {}\".format(learning_rate,\n",
    "                                                           iterations)\n",
    "\n",
    "    plot_cost_function_curve(train_X_1,\n",
    "                             train_y_norm,\n",
    "                             compute_cost,\n",
    "                             title=title,\n",
    "                             weights_list=weights_history,\n",
    "                             cost_list=cost_history)\n",
    "\n",
    "\n",
    "best_result_i = np.argmin(all_costs)\n",
    "best_w = all_w[best_result_i]\n",
    "lowest_cost = all_costs[best_result_i]\n",
    "best_params = hyper_params[best_result_i]\n",
    "\n",
    "result_str = \"Best hyperparameters\\n\"\n",
    "result_str += \"learning rate = {}\".format(best_params[0])\n",
    "result_str += \" | iterations = {}\\n\".format(best_params[1])\n",
    "result_str += \"w = {}\\n\".format(best_w.flatten())\n",
    "result_str += \"lowest validation set cost = {}\\n\".format(lowest_cost)\n",
    "\n",
    "print(result_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Com o modelo treinado e escolhidos os melhores hiper parâmetros, podemos avaliá-lo sobre o dataset de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_X_norm = standardize(test_X)\n",
    "test_y_norm = standardize(test_y)\n",
    "test_X_1 = add_feature_ones(test_X_norm)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prediction = linear_regression_prediction(test_X_1, best_w)\n",
    "prediction = (prediction * np.std(train_y)) + np.mean(train_y)\n",
    "r_2 = r_squared(test_y, prediction)\n",
    "\n",
    "plot_points_regression(test_X,\n",
    "                       test_y,\n",
    "                       title='Test data',\n",
    "                       xlabel=\"m\\u00b2\",\n",
    "                       ylabel='$',\n",
    "                       prediction=prediction,\n",
    "                       r_squared=r_2,\n",
    "                       legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente descendente estocástico\n",
    "\n",
    "Nos casos em que $N$ é um número grande, computar $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ a cada iteração se torna algo muito custoso. Uma estratégia para lidar com isso é **aproximar** $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ usando o gradiente:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\nabla_{\\mathbf{w}}J(\\mathbf{w})} = \\nabla_{\\mathbf{w}}\\frac{1}{m}\\sum_{i=1}^{m} L(h(\\mathbf{x}_{i}; \\mathbf{w}), \\; y_{i})\n",
    "\\end{equation}\n",
    "\n",
    "em que $(\\mathbf{x}_{1}, y_{1}), \\dots ,(\\mathbf{x}_{m}, y_{m})$ é uma amostragem aleatória dos dados de treinamento. A  estocasticidade surge da escolha desses $m$ dados (para que $\\hat{\\nabla_{\\mathbf{w}}J(\\mathbf{w})}$ seja um estimador não enviesado de $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ nós amostramos os $m$ dados a cada iteração). Normalmente usamos o nome **gradiente descendente estocástico** (*stochastic gradient descent* ou *online gradient descent*) quando $m=1$, e usamos o nome **minibatch stochastic gradient descent** quando $1 < m <N$ (nesse caso estamos usando apenas um pequeno lote dos dados, um *minibatch*). Usamos *batch* para referir a um *minibatch*, não confunda isso com *batch gradient descent*.\n",
    "\n",
    "\n",
    "**Stochastic gradient descent (SGD)**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * Sample a minibatch of $m$ examples from the training data.\n",
    "    * Compute the gradient estimate $\\hat{\\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))}$\n",
    "    * Apply update : $\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\hat{\\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 5)** \n",
    "Implemente o algoritmo stochastic gradient descent para a regressão linear com a taxa de apreendizado fixa. A saída da função é a mesma da função do exercício 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, w, learning_rate, num_iters, batch_size):\n",
    "    \"\"\"\n",
    "     Performs stochastic gradient descent optimization\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :param learning_rate: learning rate\n",
    "    :type learning_rate: float\n",
    "    :param num_iters: number of iterations\n",
    "    :type num_iters: int\n",
    "    :param batch_size: size of the minibatch\n",
    "    :type batch_size: int\n",
    "    :return: weights, weights history, cost history\n",
    "    :rtype: np.array(shape=(d, 1)), list, list\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError(\"falta completar a função stochastic_gradient_descent\")\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return w, weights_history, cost_history "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = time.time()\n",
    "learning_rate = 0.8\n",
    "iterations = 2000\n",
    "batch_size = 36\n",
    "w, weights_history, cost_history = stochastic_gradient_descent(train_X_1,\n",
    "                                                               train_y_norm,\n",
    "                                                               initial_w,\n",
    "                                                               learning_rate,\n",
    "                                                               iterations,\n",
    "                                                               batch_size)\n",
    "assert cost_history[-1] < cost_history[0]\n",
    "assert type(w) == np.ndarray\n",
    "assert len(weights_history) == len(cost_history)\n",
    "init = time.time() - init\n",
    "print(\"Tempo de treinamento = {:.8f}(s)\".format(init))\n",
    "print(\"Tem que ser em menos de 1.2 segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Podemos experimentar com diferentes tamanhos de batch para ver que quanto maior o tamanho do batch (mais próximo de $N$) menor a variância."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyper_params = [(0.001, 1000, 1),\n",
    "                (0.001, 1000, 10),\n",
    "                (0.001, 1000, 36)]\n",
    "all_costs = []\n",
    "\n",
    "for param in hyper_params:\n",
    "    learning_rate = param[0]\n",
    "    iterations = param[1]\n",
    "    batch_size = param[2]\n",
    "    _, weights_history, cost_history = stochastic_gradient_descent(train_X_1,\n",
    "                                                                   train_y_norm,\n",
    "                                                                   initial_w,\n",
    "                                                                   learning_rate,\n",
    "                                                                   iterations,\n",
    "                                                                   batch_size)\n",
    "    all_costs.append(cost_history)\n",
    "    title = \"Optimization landscape\\n\"\n",
    "    title += \"learning rate = {}\".format(learning_rate)\n",
    "    title += \" | iterations = {}\".format(iterations)\n",
    "    title += \" | batch size = {}\".format(batch_size)\n",
    "    plot_cost_function_curve(train_X_1,\n",
    "                             train_y_norm,\n",
    "                             compute_cost,\n",
    "                             title=title,\n",
    "                             weights_list=weights_history,\n",
    "                             cost_list=cost_history)\n",
    "\n",
    "\n",
    "_, _, cost_history_full = batch_gradient_descent(train_X_1,\n",
    "                                                 train_y_norm,\n",
    "                                                 initial_w,\n",
    "                                                 learning_rate=0.001,\n",
    "                                                 num_iters=1000)\n",
    "\n",
    "all_costs.append(cost_history_full)\n",
    "labels_size = [\"batch size = \" + str(param[2]) for param in hyper_params]\n",
    "labels_size += [\"batch size = \" + str(train_X_1.shape[0])]\n",
    "\n",
    "simple_step_plot(all_costs,\n",
    "                 \"loss\",\n",
    "                 'Training loss',\n",
    "                  figsize=(8, 8),\n",
    "                  labels=labels_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por que normalizar?\n",
    "\n",
    "O primeiro motivo para se normalizar os dados é para evitar *overflow*. Também é verdade que quando não normalizamos os dados as *features* podem apresentar diferentes escalas -- note que esse é o caso nesse dataset em que uma *feature* só tem $1$s e a outra ($m^{2}$) apresenta bastante variação. Isso influencia no gradiente de modo que a cada atualização os valores dos pesos vão mudar de modo diferente mesmo usando o mesmo *learning rate*.\n",
    "\n",
    "Isso pode ser visto quando acompanhamos a mudança nos pesos ao longo do treinamento no dataset original e no normalizado. Note como o parâmetro $\\mathbf{w}[1]$ (que pondera a feature ($m^{2}$)) muda bem mais que o parâmetro $\\mathbf{w}[0]$ quando usamos o dataset não normalizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, weights_history_norm, cost_history_norm = batch_gradient_descent(train_X_1,\n",
    "                                                                    train_y_norm,\n",
    "                                                                    initial_w,\n",
    "                                                                    learning_rate,\n",
    "                                                                    10)\n",
    "\n",
    "train_X_1_non_norm = add_feature_ones(train_X)\n",
    "\n",
    "w, weights_history, cost_history = batch_gradient_descent(train_X_1_non_norm,\n",
    "                                                          train_y,\n",
    "                                                          initial_w,\n",
    "                                                          0.000002,\n",
    "                                                          10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "w0_hist_norm = [w[0] for w in weights_history_norm]\n",
    "w1_hist_norm = [w[1] for w in weights_history_norm]\n",
    "\n",
    "\n",
    "w0mean, w0sdt, w0max, w0min = np.mean(w0_hist_norm), np.std(w0_hist_norm), np.max(w0_hist_norm), np.min(w0_hist_norm)\n",
    "w1mean, w1sdt, w1max, w1min = np.mean(w0_hist_norm), np.std(w1_hist_norm), np.max(w1_hist_norm), np.min(w1_hist_norm)\n",
    "\n",
    "print(\"\\nVariação dos pesos com o dataset normalizado\\n\")\n",
    "print(\"w[0]:\\nmean {}, std {:.2f}, max {}, min {}\".format(w0mean, w0sdt, w0max, w0min))\n",
    "print(\"w[1]:\\nmean {}, *std {:.2f}*, max {}, min {}\".format(w1mean, w1sdt, w1max, w1min))              \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "w0_hist = [w[0] for w in weights_history]\n",
    "w1_hist = [w[1] for w in weights_history]\n",
    "\n",
    "\n",
    "w0mean, w0sdt, w0max, w0min = np.mean(w0_hist), np.std(w0_hist), np.max(w0_hist), np.min(w0_hist)\n",
    "w1mean, w1sdt, w1max, w1min = np.mean(w1_hist), np.std(w1_hist), np.max(w1_hist), np.min(w1_hist)\n",
    "\n",
    "print(\"\\nVariação dos pesos com o dataset não normalizado\\n\")\n",
    "print(\"w[0]:\\nmean {}, std {:.2f}, max {}, min {}\".format(w0mean, w0sdt, w0max, w0min))\n",
    "print(\"w[1]:\\nmean {}, *std {:.2f}*, max {}, min {}\".format(w1mean, w1sdt, w1max, w1min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Um dos resultados dessa atualização em scala diferente para cada feature é a não convergência do algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simple_step_plot([cost_history_norm],\n",
    "                 \"loss\",\n",
    "                 'Training loss (normalized)')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "simple_step_plot([cost_history],\n",
    "                 \"loss\",\n",
    "                 'Training loss (non normalized)')\n",
    "\n",
    "\n",
    "\n",
    "plot_cost_function_curve(train_X_1_non_norm,\n",
    "                         train_y,\n",
    "                         compute_cost,\n",
    "                         title=\"Optimization landscape\\n(non normalized data)\",\n",
    "                         weights_list=weights_history,\n",
    "                         cost_list=cost_history,\n",
    "                         range_points=(100, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mais otimização!\n",
    "\n",
    "Há muitos outros algoritmos de otimização construídos em cima da ideia de gradiente descendente. Um bom resumo de alguns desses algoritimos pode ser encontrado [aqui](http://ruder.io/optimizing-gradient-descent/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
